{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info om modell:\n",
    "\n",
    "https://huggingface.co/ltg/norbert3-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations:\n",
    "Using Python 3.11.6\n",
    "\n",
    "python3.11 -m pip install torch\n",
    "python3.11 -m pip install transformers\n",
    "python3.11 -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Nå ønsker de seg en ny bolig.[SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-xs\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"ltg/norbert3-xs\", trust_remote_code=True)\n",
    "\n",
    "mask_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "input_text = tokenizer(\"Nå ønsker de seg en[MASK] bolig.\", return_tensors=\"pt\")\n",
    "output_p = model(**input_text)\n",
    "output_text = torch.where(input_text.input_ids == mask_id, output_p.logits.argmax(-1), input_text.input_ids)\n",
    "\n",
    "# should output: '[CLS] Nå ønsker de seg en ny bolig.[SEP]'\n",
    "print(tokenizer.decode(output_text[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Hva slags bolig ønsker de seg?[SEP] Nå ønsker de seg en moderne bolig. Den bør være romslig og godt isolert.[SEP]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-xs\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"ltg/norbert3-xs\", trust_remote_code=True)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Konteksttekst og spørsmål\n",
    "context = \"Nå ønsker de seg en moderne bolig. Den bør være romslig og godt isolert.\"\n",
    "question = \"Hva slags bolig ønsker de seg?\"\n",
    "\n",
    "# Kode for å behandle og kjøre modellen\n",
    "input_dict = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "input_ids = input_dict[\"input_ids\"].tolist()[0]\n",
    "\n",
    "outputs = model(**input_dict)\n",
    "answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "# Finner start- og sluttposisjonen for svaret i kontekstteksten\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# Konverterer ID-ene tilbake til tekst\n",
    "# answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-large\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"ltg/norbert3-large\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Hei, hvem er sjeg i aboveit?[SEP] [UNK]Stig er sjef i Aboveit. Han er en veldig flink sjef,  noen ganger ler han av vitser.[UNK][SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Konteksttekst og spørsmål\n",
    "# context = \"Værest er bra, og det er sol\"\n",
    "# question = \"Hvordan er været?\"\n",
    "context = \"\"\"\n",
    "Stig er sjef i Aboveit. Han er en veldig flink sjef,  noen ganger ler han av vitser.\n",
    "\"\"\"\n",
    "question = \"Hei, hvem er sjeg i aboveit?\"\n",
    "\n",
    "\n",
    "# Kode for å behandle og kjøre modellen\n",
    "input_dict = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "input_ids = input_dict[\"input_ids\"].tolist()[0]\n",
    "\n",
    "outputs = model(**input_dict)\n",
    "answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "# Finner start- og sluttposisjonen for svaret i kontekstteksten\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# Konverterer ID-ene tilbake til tekst for bare svaret\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "# answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 start scores: torch.return_types.topk(\n",
      "values=tensor([[0., 0., 0., 0., 0.]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[0, 1, 2, 3, 4]]))\n",
      "Top 5 end scores: torch.return_types.topk(\n",
      "values=tensor([[0., 0., 0., 0., 0.]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[0, 1, 2, 3, 4]]))\n",
      "Svar: [CLS]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Kode for å behandle og kjøre modellen\n",
    "input_dict = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "input_ids = input_dict[\"input_ids\"].tolist()[0]\n",
    "\n",
    "outputs = model(**input_dict)\n",
    "answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "# Skriver ut de 5 beste start- og sluttscorene\n",
    "print(\"Top 5 start scores:\", torch.topk(answer_start_scores, 5))\n",
    "print(\"Top 5 end scores:\", torch.topk(answer_end_scores, 5))\n",
    "\n",
    "# Finner start- og sluttposisjonen for svaret i kontekstteksten\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# Konverterer ID-ene tilbake til tekst for bare svaret\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "print(\"Svar:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "# model = AutoTokenizer.from_pretrained('WizardLM/WizardLM-70B-V1.0')\n",
    "# tokenizer = AutoModelForQuestionAnswering.from_pretrained('WizardLM/WizardLM-70B-V1.0')\n",
    "\n",
    "# model = AutoTokenizer.from_pretrained('WizardLM/WizardCoder-Python-34B-V1.0')\n",
    "# tokenizer = AutoModelForQuestionAnswering.from_pretrained('WizardLM/WizardCoder-Python-34B-V1.0')\n",
    "# https://huggingface.co/WizardLM/WizardLM-70B-V1.0\n",
    "\n",
    "# model_name = \"albert-large-v2\"  # You can replace this with 'albert-large-v2', 'albert-xlarge-v2', etc.\n",
    "# model_name = \"WizardLM/WizardLM-70B-V1.0\"  \n",
    "\n",
    "# tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "# model = AlbertForQuestionAnswering.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ltg/norbert3-large\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"ltg/norbert3-large\", trust_remote_code=True)\n",
    "# Display the model's architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text, question):\n",
    "    # Tokenize the input text and question\n",
    "    inputs = tokenizer(question, text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    # Get the start and end scores for the answer\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the question is: stig\n"
     ]
    }
   ],
   "source": [
    "# text = \"\"\"\n",
    "# The Eiffel Tower, located in Paris, France, is one of the most iconic landmarks in the world. It was designed by Gustave Eiffel and completed in 1889. The tower stands at a height of 324 meters and was the tallest man-made structure in the world at the time of its completion.\n",
    "# \"\"\"\n",
    "context = \"\"\"\n",
    "Stig is the boss at Aboveit. He is a very capable boss, and sometimes he laughs at jokes. Always use a capital letter at the beginning when referring to people..\n",
    "\"\"\"\n",
    "question = \"Hello, who is the boss at Aboveit?\"\n",
    "\n",
    "# Get the answer to the question\n",
    "answer = get_answer(context, question)\n",
    "print(f\"The answer to the question is: {answer}\")\n",
    "# Output: The answer to the question is: Gustave Eiffel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 720/720 [00:00<00:00, 8.30MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 92.7k/92.7k [00:00<00:00, 862kB/s]\n",
      "model-00001-of-00019.safetensors: 100%|██████████| 4.89G/4.89G [05:10<00:00, 15.7MB/s]\n",
      "model-00002-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [04:30<00:00, 18.4MB/s]\n",
      "model-00003-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [04:59<00:00, 16.6MB/s]\n",
      "model-00004-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [07:43<00:00, 10.6MB/s]\n",
      "model-00005-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [05:29<00:00, 15.1MB/s]\n",
      "model-00006-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [07:02<00:00, 11.8MB/s]\n",
      "model-00007-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [06:31<00:00, 12.5MB/s]\n",
      "model-00008-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [05:53<00:00, 14.1MB/s]\n",
      "Downloading shards:  42%|████▏     | 8/19 [47:23<1:08:08, 371.69s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                            #  device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 720/720 [00:00<00:00, 8.30MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 92.7k/92.7k [00:00<00:00, 862kB/s]\n",
      "model-00001-of-00019.safetensors: 100%|██████████| 4.89G/4.89G [05:10<00:00, 15.7MB/s]\n",
      "model-00002-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [04:30<00:00, 18.4MB/s]\n",
      "model-00003-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [04:59<00:00, 16.6MB/s]\n",
      "model-00004-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [07:43<00:00, 10.6MB/s]\n",
      "model-00005-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [05:29<00:00, 15.1MB/s]\n",
      "model-00006-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [07:02<00:00, 11.8MB/s]\n",
      "model-00007-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [06:31<00:00, 12.5MB/s]\n",
      "model-00008-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [05:53<00:00, 14.1MB/s]\n",
      "Downloading shards:  42%|████▏     | 8/19 [47:23<1:08:08, 371.69s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                            #  device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
