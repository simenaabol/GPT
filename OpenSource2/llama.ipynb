{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info om modell:\n",
    "\n",
    "https://huggingface.co/ltg/norbert3-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations:\n",
    "Using Python 3.11.6\n",
    "\n",
    "python3.11 -m pip install torch\n",
    "python3.11 -m pip install transformers\n",
    "python3.11 -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/simenaabol/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "model.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 37.4MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [07:33<00:00, 22.0MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [02:33<00:00, 22.8MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [10:07<00:00, 303.89s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.98s/it]\n",
      "generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 827kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "# 10 min å laste ned\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "\"text-generation\",\n",
    "model=model,\n",
    "tokenizer=tokenizer,\n",
    "torch_dtype=torch.float16,\n",
    "device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 21, but `max_length` is set to 21. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have tomatoes, basil and cheese at home. What can I cook for dinner?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "do_sample=True,\n",
    "top_k=10,\n",
    "num_return_sequences=1,\n",
    "eos_token_id=tokenizer.eos_token_id,\n",
    "max_length=21,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the question is: stig\n"
     ]
    }
   ],
   "source": [
    "# context = \"\"\"\n",
    "# Stig is the boss at Aboveit. He is a very capable boss, and sometimes he laughs at jokes. \n",
    "# \"\"\"\n",
    "# question = \"Hello, who is not the boss at Aboveit?\"\n",
    "# # question = \"Hei, hvem er ikke sjefen hos Aboveit\"\n",
    "\n",
    "\n",
    "# # Get the answer to the question\n",
    "# answer = get_answer(context, question)\n",
    "# print(f\"The answer to the question is: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
