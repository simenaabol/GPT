{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "\n",
    "# model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "#  1.5 sek\n",
    "#  Engelsk\n",
    "#  1/ 2 poeng\n",
    "#  Norsk\n",
    "#  2/ 2 poeng\n",
    "\n",
    "\n",
    "# model_name = 'ltg/norbert3-large'\n",
    "#  2.3 sek\n",
    "#  Engelsk\n",
    "#  0?/ 2 poeng\n",
    "#  Norsk\n",
    "#  0?/ 2 poeng\n",
    "\n",
    "# model_name = 'microsoft/DialoGPT-small'\n",
    "#  0 sek\n",
    "#  Engelsk\n",
    "#  0/ 2 poeng\n",
    "#  Norsk\n",
    "#  0/ 2 poeng\n",
    "\n",
    "\n",
    "model_name = ''\n",
    "#  0 sek\n",
    "#  Engelsk\n",
    "#  0/ 2 poeng\n",
    "#  Norsk\n",
    "#  0/ 2 poeng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_name = 'lmsys/vicuna-13b-v1.5-16k'\n",
    "#   sek\n",
    "#  Engelsk\n",
    "#  0?/ 2 poeng\n",
    "#  Norsk\n",
    "#  0?/ 2 poeng\n",
    "\n",
    "# model_name = 'pankajmathur/orca_mini_v3_7b'\n",
    "#   sek\n",
    "#  Engelsk\n",
    "#  0?/ 2 poeng\n",
    "#  Norsk\n",
    "#  0?/ 2 poeng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering, AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,  trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(text, question):\n",
    "    # Tokenize the input text and question\n",
    "    inputs = tokenizer(question, text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    # Get the start and end scores for the answer\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "    # answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOULD BE 'Stig':        [CLS] hello, who is the boss at aboveit? [SEP] stig is the boss at aboveit. he is a very capable boss, and sometimes he laughs at jokes. [SEP]\n",
      "SHOULD NOT BE 'Stig':    [CLS] hello, who is the boss at aboveit? [SEP] stig is not the boss at aboveit. he is a very capable boss, and sometimes he laughs at jokes. [SEP]\n",
      "Bør være 'Stig':         [CLS] hei, hvem er sjefen i aboveit? [SEP] stig er sjef i aboveit. han er en veldig dyktig sjef, og av og til ler han av vitser. [SEP]\n",
      "Bør ikke være 'Stig':    [CLS] hei, hvem er sjefen i aboveit? [SEP] stig er ikke sjef i aboveit. han er en veldig dyktig sjef, og av og til ler han av vitser. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Engelsk\n",
    "context = \"\"\"\n",
    "Stig is the boss at Aboveit. He is a very capable boss, and sometimes he laughs at jokes.\n",
    "\"\"\"\n",
    "question = \"Hello, who is the boss at Aboveit?\"\n",
    "answer = get_answer(context, question)\n",
    "print(f\"SHOULD BE 'Stig':        {answer}\")\n",
    "\n",
    "context = \"\"\"\n",
    "Stig is not the boss at Aboveit. He is a very capable boss, and sometimes he laughs at jokes.\n",
    "\"\"\"\n",
    "question = \"Hello, who is the boss at Aboveit?\"\n",
    "answer = get_answer(context, question)\n",
    "print(f\"SHOULD NOT BE 'Stig':    {answer}\")\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Stig er sjef i Aboveit. Han er en veldig dyktig sjef, og av og til ler han av vitser.\n",
    "\"\"\"\n",
    "question = \"Hei, hvem er sjefen i Aboveit?\"\n",
    "answer = get_answer(context, question)\n",
    "print(f\"Bør være 'Stig':         {answer}\")\n",
    "\n",
    "context = \"\"\"\n",
    "Stig er ikke sjef i Aboveit. Han er en veldig dyktig sjef, og av og til ler han av vitser.\n",
    "\"\"\"\n",
    "# bert-large-cased-whole-word-masking-finetuned-squad\n",
    "answer = get_answer(context, question)\n",
    "print(f\"Bør ikke være 'Stig':    {answer}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,  trust_remote_code=True)\n",
    "\n",
    "\n",
    "# AutoModelForCausalLM, som er designet for årsakssammenheng i språkmodellering (causal language modeling) og genererer tekst basert på tidligere kontekst. Denne modelltypen gir ikke start_logits og end_logits, som er nødvendig for spørsmål-svar-oppgaver.\n",
    "\n",
    "# For spørsmål-svar-oppgaver, bør du bruke en modell som er spesifikt finjustert for dette formålet, som AutoModelForQuestionAnswering. Her er hvordan du kan endre koden for å bruke riktig modelltype:\n",
    "\n",
    "\n",
    "\n",
    "# Angi stien hvor du vil lagre modellen og tokenizeren\n",
    "save_directory = model_name\n",
    "\n",
    "# Lagre modellen\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Lagre tokenizeren\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
