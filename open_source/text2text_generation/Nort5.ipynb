{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info om modell:\n",
    "\n",
    "https://huggingface.co/ltg/norbert3-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations:\n",
    "Using Python 3.11.6\n",
    "\n",
    "python3.11 -m pip install torch\n",
    "python3.11 -m pip install transformers\n",
    "python3.11 -m pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ltg/nort5-large-lm\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ltg/nort5-large-lm\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you?\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"translate English to German: How old are you?\"\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you? How old are you?'}]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"Hei, klarer du å lage historier om katter?\"\n",
    "# res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en katt som heter \"Katten\" og jeg har en\n"
     ]
    }
   ],
   "source": [
    "input_to_model = \"Hei, klarer du å lage historier om katter?\"\n",
    "# res = pipe(input_to_model, max_new_tokens=(200 + len(input_to_model)), return_tensors=False )\n",
    "res = pipe(input_to_model, return_tensors=False )\n",
    "print(res[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS]ˈoppvarming, det vil si at det skjer en endring i temperaturen i et medium, f.eks. en ovn eller en radiator, slik at den blir varmere eller kaldere, eller at den blir varmere eller kaldere, eller at den blir'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Brukseksempel: Elektrisk oppvarming. Definisjonen på ordet oppvarming er (Wikipedia) \"\n",
    "encoding = tokenizer(sentence)\n",
    "\n",
    "input_tensor = torch.tensor([encoding.input_ids])\n",
    "output_tensor = model.generate(input_tensor, max_new_tokens=50, num_beams=4, do_sample=False)\n",
    "tokenizer.decode(output_tensor.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS] Her kan du lese mer om hvordan du kan lage en historie om katter. Her kan du lese mer om hvordan du kan lage en historie om hunder. Her kan du lese mer om hvordan du kan lage en historie om hunder. Her kan du lese mer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Klarer du å lage en historier om katter?\"\n",
    "encoding = tokenizer(sentence)\n",
    "\n",
    "input_tensor = torch.tensor([encoding.input_ids])\n",
    "output_tensor = model.generate(input_tensor, max_new_tokens=50, num_beams=4, do_sample=False)\n",
    "tokenizer.decode(output_tensor.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS] Hva kan jeg lage med ost og hvetemel? Hva kan jeg lage med ost og hvetemel? Hva kan jeg lage med ost og hvetemel? Hva kan jeg lage med ost og hvetemel? Hva kan jeg lage med ost og hvetemel? Hva kan jeg lage med'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hva kan jeg lage med med ost og hvetemel?\"\n",
    "encoding = tokenizer(sentence)\n",
    "\n",
    "input_tensor = torch.tensor([encoding.input_ids])\n",
    "output_tensor = model.generate(input_tensor, max_new_tokens=50, num_beams=4, do_sample=False)\n",
    "tokenizer.decode(output_tensor.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Hva bør man huske på når man går tur i skogen \"\n",
    "sentence = \"Brukseksempel: Elektrisk oppvarming. Definisjonen på ordet oppvarming er (NRK) \"\n",
    "encoding = tokenizer(sentence)\n",
    "\n",
    "input_tensor = torch.tensor([encoding.input_ids])\n",
    "output_tensor = model.generate(input_tensor, max_new_tokens=50, num_beams=4, do_sample=False)\n",
    "tokenizer.decode(output_tensor.squeeze())\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Stig is the boss at Aboveit. He is a very capable boss, and sometimes he laughs at jokes. Always use a capital letter at the beginning when referring to people..\n",
    "\"\"\"\n",
    "question = \"Hello, who is the boss at Aboveit?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
